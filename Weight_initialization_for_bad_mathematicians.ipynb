{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight initialization for bad mathematicians\n",
    "=================================\n",
    "\n",
    "\n",
    "Well I like math, I'm just bad at it, very bad. But this should not stop me to understand deep learning, actually the best deep learning practitioner that I know, Jeremy Howard also proclaims he is a bad mathematician.  \n",
    "\n",
    "Weight initialisation matters, it matters A LOT! This simple \"trick\" can be the difference between convergence and total failure.  \n",
    "\n",
    "As described by this great paper, the loss surface of a neural network can be very chaotic (and as explained by the paper, this heavily depends on the architecture you're using).  \n",
    "\n",
    "t loss landscapes for all the networks considered seem to be partitioned into a well-defined region of low loss value and convex contours, surrounded by a well-defined region of high loss value and non-convex contours.  \n",
    "\n",
    "Near the local minimum the loss structure is well behaved and very smooth, but as we move away to the regions of high loss value, the contours start to become sharp and chaotic.\n",
    "With a good weight initialization strategy, your initial loss will likely lie in the well-behaved region, and your model might never see the chaotic space that lies on the high loss region.  \n",
    "\n",
    "If your model already starts in this very chaotic high loss space, your gradients will be very uninformative and and training will be impossible, your model will keep jumping here and there and will not be able to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a very simple example to see the harm that a very normal initialization can cause:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let's import the necessary dependencies and define some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monkey patching some utility functions, you can ignore these\n",
    "tf.Tensor.mean = lambda o, **kwargs: tf.reduce_mean(o, **kwargs)\n",
    "tf.Tensor.pow = lambda o, p, **kwargs: tf.pow(o, p, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(x, w): return x@w\n",
    "def relu(x): return tf.math.maximum(0, x)\n",
    "def stats(x):\n",
    "  mean, std = float(tf.reduce_mean(x)), float(tf.math.reduce_std(x))**2\n",
    "  print(f'Mean: {mean}\\nVar: {std}')\n",
    "def forward(x, ws, act=None):\n",
    "  for w in ws: x = act(lin(x, w)) if act else lin(x, w)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our inputs will come from a normal distribution since data is almost always normalized in ML.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal((1, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our weights will also come from a normal distribution. Very harmless huh? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.random.normal((1000, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a forward pass and take a look on what happens with the mean and std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: -0.2934545874595642\n",
      "Var: 1067.616436898301\n"
     ]
    }
   ],
   "source": [
    "stats(forward(x, [w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! After the very first layer the variance is already huge! You think that's not concerning enough? Let's see what happens if we go deep and stack 50 of those layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: nan\n",
      "Var: nan\n"
     ]
    }
   ],
   "source": [
    "ws = [tf.random.normal((1000, 1000)) for _ in range(50)]\n",
    "stats(forward(x, ws))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you go, it already exploded. What happens is that our activations get so huge that the computer cannot keep track anymore, and the we get `Not A Number`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why we have such high numbers for variance?  \n",
    "It ends up that when we are calculating the matrix multiplication `x@w` we sum 512 products of one element of `w` by one element of `x`. The mean of each of this little products is 0, so the final mean tends to be close to 0, but the variance is 1 for each of them, so when we sum all together all of the single variances adds up, and we end up with a variance that tends towards the number of units we have in the layer.  \n",
    "As we stack more layers the variance keeps growing exponentially.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution? Scale the weights in such a way that the variance of the output of each layer is 1. This way we can stack as many layers as we want and the output of every layer will have a variance of one.\n",
    "\n",
    "So the question becomes, how to scale this layers?  \n",
    "Well, we already concluded that the output variance is going to be equal to the number of inputs to the layer, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that variance is the average of how far our values are from the mean, let's try doing that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=215, shape=(), dtype=float32, numpy=-1.7642975e-08>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal((1000,))\n",
    "(x - x.mean()).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, so our variance is zero? Well... **NO**.  \n",
    "\n",
    "Some values are going to be greater than the mean and some are going to be smaller, by definition the mean is going to have the same **sum of distances** from the right and from the left side, so when we sum all of these little distances from the mean together, we can zero.\n",
    "\n",
    "What kills us here is that some values are positives and some are negatives, so what we do? We square all the distances so they're are always positive:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=229, shape=(), dtype=float32, numpy=0.9731436>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x - x.mean()).pow(2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thus we have variance, easy peasy. And this is the mathematical formula for what we just coded:  \n",
    "\n",
    "$$\\frac{1}{n}{\\displaystyle\\sum_{i=1}^{n}(x_i - \\mu)^2}$$  \n",
    "\n",
    "Remember our original problem, we want to change all $x_i$ so we can modify the variance. How would we make the variance of the previous example 10 times smaller? We can try dividing all $x$ by 10:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=238, shape=(), dtype=float32, numpy=0.009731436>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = x/10\n",
    "(x1 - x1.mean()).pow(2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that made the variance 100 times smaller, that is because we are squaring all of our values.  \n",
    "\n",
    "So if we want the make our variance 10 times smaller we have to divide all our values with $\\sqrt{10}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=247, shape=(), dtype=float32, numpy=0.097314365>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = x/math.sqrt(10)\n",
    "(x1 - x1.mean()).pow(2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our original problem the variance of the output was equal to the number of inputs to that layer. We want the variance to be `n_inputs` smaller, so we just have to divide our weights with $\\sqrt{\\text{n_inputs}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal((1, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.039295364171266556\n",
      "Var: 1.0486676543465023\n"
     ]
    }
   ],
   "source": [
    "w = tf.random.normal((1000, 1000)) / math.sqrt(1000)\n",
    "stats(forward(x, [w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have it! The variance of the output is 1! Let's try doing the same with 50 stacked layers:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: -0.05769466981291771\n",
      "Var: 0.9657826813236312\n"
     ]
    }
   ],
   "source": [
    "ws = [tf.random.normal((1000, 1000))/math.sqrt(1000) for _ in range(50)]\n",
    "stats(forward(x, ws))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talk about relu and kaiming init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (window.IPython && IPython.notebook.kernel) IPython.notebook.kernel.execute('jovian.utils.jupyter.get_notebook_name_saved = lambda: \"' + IPython.notebook.notebook_name + '\"')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Saving notebook..\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jovian\n",
    "jovian.commit(secret=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
